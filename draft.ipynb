{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install secedgar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3vZU3FC0cZBL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create request header\n",
        "headers = {'User-Agent': \"Kevin.Peterson@bainbridge.com\"}\n",
        "\n",
        "ticker = 'NVDA'\n",
        "filing_type = '10-Q' # 10-Q, 10-K\n",
        "\n",
        "latest_nth_report = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filing = WebScrape(headers, ticker, filing_type, latest_nth_report)\n",
        "# final_text = filing.extract_mda_section()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get all companies data\n",
        "companyTickers = requests.get(\n",
        "    \"https://www.sec.gov/files/company_tickers.json\",\n",
        "    headers=headers\n",
        "    )\n",
        "\n",
        "ticker_json = pd.DataFrame.from_dict(companyTickers.json(),orient='index')\n",
        "\n",
        "# add leading zeros to CIK\n",
        "ticker_json['cik_str'] = ticker_json['cik_str'].astype(str).str.zfill(10)\n",
        "ticker_cik_dic = dict(zip(ticker_json['ticker'], ticker_json['cik_str']))\n",
        "cik = ticker_cik_dic[ticker]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "filingMetadata = requests.get(\n",
        "    f'https://data.sec.gov/submissions/CIK{cik}.json',\n",
        "    headers=headers\n",
        "    )\n",
        "\n",
        "recent_forms = pd.DataFrame.from_dict(\n",
        "             filingMetadata.json()['filings']['recent']\n",
        "             )\n",
        "recent_forms['accessionNumber'] = recent_forms['accessionNumber'].str.replace(\"-\",\"\")\n",
        "target_forms = recent_forms.loc[recent_forms.form == filing_type].reset_index(drop=True)\n",
        "target_forms.sort_values('reportDate', ascending=False)\n",
        "\n",
        "accesion_number = target_forms.loc[latest_nth_report,'accessionNumber']\n",
        "document_suffix = target_forms.loc[latest_nth_report,'primaryDocument']\n",
        "url = f'https://www.sec.gov/Archives/edgar/data/{cik}/{accesion_number}/{document_suffix}'\n",
        "# get company facts data\n",
        "single_filing = requests.get(url, headers=headers)\n",
        "filing_summary_response = single_filing.content.decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# def extract_table_sections_from_response(response):\n",
        "#     \"\"\"\n",
        "#     Uses response.content (with the lxml parser) to preserve style attributes,\n",
        "#     then finds all <div> elements whose style attribute matches the regular expression for \"margin-bottom:1pt\".\n",
        "#     For each matching div, it extracts its inner HTML (all direct children) as an HTML fragment and removes the div.\n",
        "    \n",
        "#     Args:\n",
        "#         response (requests.Response): The HTTP response from requests.get.\n",
        "    \n",
        "#     Returns:\n",
        "#         tuple: (modified_soup, list_of_table_section_fragments)\n",
        "#             - modified_soup: a BeautifulSoup object (with the extracted divs removed)\n",
        "#             - list_of_table_section_fragments: a list of HTML strings representing each extracted table section.\n",
        "#     \"\"\"\n",
        "#     import warnings\n",
        "#     warnings.filterwarnings(\"ignore\")\n",
        "    \n",
        "#     # Use the lxml parser to better preserve style attributes.\n",
        "#     soup = BeautifulSoup(response.content, \"lxml\")\n",
        "#     table_sections = []\n",
        "#     candidate_divs = []\n",
        "#     margin_bottom_re = re.compile(r\"margin-bottom\\s*:\\s*1pt;?\", re.IGNORECASE)\n",
        "    \n",
        "#     all_divs = soup.find_all(\"div\")\n",
        "#     for div in all_divs:\n",
        "#         style = div.get(\"style\", \"\")\n",
        "#         if style and margin_bottom_re.search(style):\n",
        "#             candidate_divs.append(div)\n",
        "    \n",
        "#     # For each candidate, extract its inner HTML and remove the div.\n",
        "#     for idx, div in enumerate(candidate_divs, start=1):\n",
        "#         inner_xml = div.decode_contents().strip()\n",
        "#         table_sections.append(inner_xml)\n",
        "#         div.decompose()\n",
        "    \n",
        "#     return soup, table_sections\n",
        "\n",
        "def extract_table_sections_from_response(response):\n",
        "    \"\"\"Extract table sections from HTML response and remove them from the document unless they contain keywords.\"\"\"\n",
        "    soup = BeautifulSoup(response.content, \"lxml\")\n",
        "    table_sections = []\n",
        "    table_style_re = re.compile(r\"margin-bottom\\s*:\\s*\\d+pt\", re.IGNORECASE)\n",
        "    keyword_re = re.compile(r\"Item\\s*\\d|Managementâ€™s Discussion and Analysis\", re.IGNORECASE)\n",
        "    \n",
        "    all_tables = soup.find_all(\"table\")\n",
        "    for table in all_tables:\n",
        "        if table_style_re.search(table.get(\"style\", \"\")):\n",
        "            # Check if the table contains key phrases in its spans\n",
        "            if any(keyword_re.search(span.get_text()) for span in table.find_all(\"span\")):\n",
        "                continue  # Skip removal if key phrase is found\n",
        "            \n",
        "            table_sections.append(table.decode_contents().strip())\n",
        "            table.decompose()\n",
        "    \n",
        "    return soup, table_sections\n",
        "\n",
        "\n",
        "def find_last_div_containing_text(soup, text_pattern):\n",
        "    \"\"\"\n",
        "    Finds the last <div> element (with a style attribute) that contains text matching the given pattern.\n",
        "    \n",
        "    Args:\n",
        "        soup (BeautifulSoup): The parsed HTML document.\n",
        "        text_pattern (str): The regex pattern to search for.\n",
        "    \n",
        "    Returns:\n",
        "        BeautifulSoup element or None: The last matching div, or None if not found.\n",
        "    \"\"\"\n",
        "    matching_divs = [div for div in soup.find_all(\"div\", style=True)\n",
        "                     if re.search(text_pattern, div.get_text(), re.IGNORECASE)]\n",
        "    # print(\"DEBUG: find_last_div_containing_text found\", len(matching_divs), \"matching divs.\")\n",
        "    return matching_divs[-1] if matching_divs else None\n",
        "\n",
        "def extract_text_until_next_item(start_div, pattern):\n",
        "    \"\"\"\n",
        "    Starting at start_div, extracts text from that div until reaching a sibling <div> whose text matches the stop pattern.\n",
        "    \n",
        "    Args:\n",
        "        start_div (BeautifulSoup element): The starting <div> (e.g., the MD&A header).\n",
        "        pattern (str): Regex pattern that marks the stopping point (e.g., the next item header).\n",
        "    \n",
        "    Returns:\n",
        "        str: Concatenated text from start_div until the stopping condition.\n",
        "    \"\"\"\n",
        "    if not start_div:\n",
        "        return \"Desired section not found.\"\n",
        "    \n",
        "    item_text = []\n",
        "    current = start_div\n",
        "    while current:\n",
        "        item_text.append(current.get_text(strip=True))\n",
        "        current = current.find_next_sibling(\"div\")\n",
        "        if current and re.search(pattern, current.get_text(), re.IGNORECASE):\n",
        "            break\n",
        "    full_text = \" \".join(item_text).strip().replace(\"\\xa0\", \"\")\n",
        "    return full_text\n",
        "\n",
        "def retain_text_after_last_occurrence(text, pattern):\n",
        "    \"\"\"\n",
        "    Retains only the text following the last occurrence of the specified pattern.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The full extracted text.\n",
        "        pattern (str): Regex pattern to locate the marker.\n",
        "    \n",
        "    Returns:\n",
        "        str: The text following the last occurrence of the marker.\n",
        "    \"\"\"\n",
        "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
        "    if matches:\n",
        "        # Slice the text after the end of the last match.\n",
        "        return text[matches[-1].end()-5:].strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine the regex patterns based on the form type\n",
        "if target_forms.loc[latest_nth_report, 'form'] == '10-Q':\n",
        "    current_item_pattern = r\"Item\\s*2[^\\w]+Management.*?Discussion.*?and.*?Analysis\"\n",
        "    next_item_pattern = r\"Item\\s*3\"\n",
        "    last_item_pattern = r\"Item\\s*2\"\n",
        "elif target_forms.loc[latest_nth_report, 'form'] == '10-K':\n",
        "    current_item_pattern = r\"Item\\s*7[^\\w]+Management.*?Discussion.*?and.*?Analysis\"\n",
        "    next_item_pattern = r\"Item\\s*8\"\n",
        "    last_item_pattern = r\"Item\\s*7\"\n",
        "\n",
        "# Fetch and parse the document\n",
        "try:\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "except requests.RequestException as e:\n",
        "    print(f\"Error fetching URL: {e}\")\n",
        "    response = None\n",
        "\n",
        "# Step 1: Extract table sections and remove them from the HTML\n",
        "modified_soup, table_fragments = extract_table_sections_from_response(response)    \n",
        "\n",
        "# Step 2: Find the last <div> containing the MD&A header text.\n",
        "last_item_div = find_last_div_containing_text(modified_soup, current_item_pattern)\n",
        "\n",
        "# Step 3: Extract text from the found div until the next item (e.g., \"Item 3\")\n",
        "extracted_text = extract_text_until_next_item(last_item_div, next_item_pattern)\n",
        "\n",
        "# Step 3: Retain only the text after the last occurrence of the marker (current item pattern)\n",
        "final_text = retain_text_after_last_occurrence(extracted_text, last_item_pattern)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "final_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "filingMetadata = requests.get(\n",
        "    f'https://data.sec.gov/submissions/CIK{cik}.json',\n",
        "    headers=headers\n",
        "    )\n",
        "\n",
        "# dictionary to dataframe\n",
        "allForms = pd.DataFrame.from_dict(\n",
        "             filingMetadata.json()['filings']['recent']\n",
        "             )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # filing metadata\n",
        "# companyFacts.json()['facts']['dei'][\n",
        "#     'EntityCommonStockSharesOutstanding']\n",
        "# companyFacts.json()['facts']['dei'][\n",
        "#     'EntityCommonStockSharesOutstanding'].keys()\n",
        "# companyFacts.json()['facts']['dei'][\n",
        "#     'EntityCommonStockSharesOutstanding']['units']\n",
        "# companyFacts.json()['facts']['dei'][\n",
        "#     'EntityCommonStockSharesOutstanding']['units']['shares']\n",
        "# companyFacts.json()['facts']['dei'][\n",
        "#     'EntityCommonStockSharesOutstanding']['units']['shares'][0]\n",
        "\n",
        "# # concept data // financial statement line items\n",
        "# companyFacts.json()['facts']['us-gaap']\n",
        "# companyFacts.json()['facts']['us-gaap'].keys()\n",
        "\n",
        "# # different amounts of data available per concept\n",
        "# companyFacts.json()['facts']['us-gaap']['AccountsPayable']\n",
        "# companyFacts.json()['facts']['us-gaap']['Revenues']\n",
        "# companyFacts.json()['facts']['us-gaap']['Assets']\n",
        "\n",
        "# # get company concept data\n",
        "# companyConcept = requests.get(\n",
        "#     (\n",
        "#     f'https://data.sec.gov/api/xbrl/companyconcept/CIK{cik}'\n",
        "#      f'/us-gaap/Assets.json'\n",
        "#     ),\n",
        "#     headers=headers\n",
        "#     )\n",
        "\n",
        "# # review data\n",
        "# companyConcept.json().keys()\n",
        "# companyConcept.json()['units']\n",
        "# companyConcept.json()['units'].keys()\n",
        "# companyConcept.json()['units']['USD']\n",
        "# companyConcept.json()['units']['USD'][0]\n",
        "\n",
        "# # parse assets from single filing\n",
        "# companyConcept.json()['units']['USD'][0]['val']\n",
        "\n",
        "# # get all filings data \n",
        "# assetsData = pd.DataFrame.from_dict((\n",
        "#                companyConcept.json()['units']['USD']))\n",
        "\n",
        "# # review data\n",
        "# assetsData.columns\n",
        "# assetsData.form\n",
        "\n",
        "# # get assets from 10Q forms and reset index\n",
        "# assets10Q = assetsData[assetsData.form == '10-Q']\n",
        "# assets10Q = assets10Q.reset_index(drop=True)\n",
        "\n",
        "# # plot \n",
        "# assets10Q.plot(x='end', y='val')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
